---
title: "Data Dissemination: Part 1"
format: 
  html:
    toc: true
    code-line-numbers: true
editor_options: 
  chunk_output_type: console
---


```{r}
#| echo: false

exercise_number <- 1

```

![A data lifecycle diagram emphasizing the 'Data dissemination' stage in green, with the remaining stages shown in lighter colors. This stage involves creating written, digital, and verbal products to relevant stakeholders.](www/images/data-lifecycle-dissemination.png){#fig-data-analysise fig-align="center" width=100%}

For this week, we will learn the different aspects of disseminating data while considering security, privacy, ethics, and equity. In part 1, we focus on the legal and policy side before disseminating your work.

## Quick recap on week 4

### Privacy-utility tradeoff

There is often a tension between privacy and data utility. This tension is referred to in the privacy literature as the **privacy-utility tradeoff**.

::: callout-tip
### Data Utility

**Data utility,** quality, accuracy, or usefulness is how practically useful or accurate to the data are for research and analysis purposes.
:::

![*Generally*, as privacy increases, the image quality (utility) decreases, and vice versa.](www/images/04_privacy-utility-tradeoff.png){#fig-tradeoff fig-align="center"}

### Assessing utility

Generally there are three ways to measure utility of the data:

::: callout-tip
### General utility

**General utility**, sometimes called global utility, measures the univariate and multivariate distributional similarity between the confidential data and the public data (e.g., sample means, sample variances, and the variance-covariance matrix).
:::

::: callout-tip
### Specific utility

**Specific utility**, sometimes called outcome specific utility, measures the similarity of results for a specific analysis (or analyses) of the confidential and public data (e.g., comparing the coefficients in regression models).
:::

::: callout-tip
### Fit-for-purpose

**Fit-for-purpose**, are something in between the previous two utility metric types to quickly assess the quality of the public data compared to the confidential data. They are not global measures, because they focus on certain features of the data, but may not be specific to an analysis that data users and stakeholders are interested in like analysis-specific utility metrics.
:::

### Assessing disclosure risk

Note that most thresholds for acceptable disclosure risk are often determined by law.

There are generally three kinds of disclosure risk:

::: callout-tip
#### Identity disclosure

**Identity disclosure** occurs if the data intruder associates a known individual with a public data record (e.g., a record linkage attack or when a data adversary combines one or more external data sources to identify individuals in the public data).
:::

::: callout-tip
### Attribute disclosure

**Attribute disclosure** occurs if the data intruder determines new characteristics (or attributes) of an individual based on the information available through public data or statistics (e.g., if a dataset shows that all people age 50 or older in a city are on Medicaid, then the data adversary knows that any person in that city above age 50 is on Medicaid). This information is learned without idenfying a specific individual in the data!
:::

::: callout-tip
### Inferential disclosure

**Inferential disclosure** occurs if the data intruder predicts the value of some characteristic from an individual more accurately with the public data or statistic than would otherwise have been possible (e.g., if a public homeownership dataset reports a high correlation between the purchase price of a home and family income, a data adversary could infer another person's income based on purchase price listed on Redfin or Zillow).
:::

### Hidden stories in news reports

![Piled HIgher and Deeper comic, "The Science News Cycle."](www/images/04_phd-comics-news.gif){#fig-science-news-cycle fig-align="center"}

There are often two sides to every story, especially when it comes to how people collect, store, transfer, and analyze data. In this in-class activity, we will apply the ideas and concepts we've learned so far to answer a question in multiple ways that are technically correct, even if the different answers contradict one another.

### Week 5 Assignment

#### Read

- Chapter 6: What Data Privacy Laws Exist?

#### Optional additional read

- [Do No Harm Guide: Applying Equity Awareness in Data Visualization](https://www.urban.org/research/publication/do-no-harm-guide-applying-equity-awareness-data-visualization).

#### Optional watch

- [Data Brokers: Last Week Tonight with John Oliver (HBO)](https://www.youtube.com/watch?v=wqn3gR1WTcA)

#### Collect data
For one day, record how often private companies collect *your data* from the moment you wake up to the moment you go to bed. For example, opening a social media app or browsing the web.

#### Write (600 to 1200 words)

Based on your data log, answer the following questions:

- Is any of the information collected by private companies protected by a state or federal law?
- What could a private company do with that information for the public good?
- What could a private company do with that information that may harm or violate your personal privacy?
- What are the data equity and ethical impacts of collecting this information?

::: callout
#### [`r paste("Class Activity", exercise_number)`]{style="color:#1696d2;"}

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

Throughout this course, we’ve been exploring security, privacy, and ethics across the data lifecycle. In this assignment, you examined how various entities (most often private companies) collected your information over the course of a single day. Based on your experience...

1. Looking back at your data log, was there anything that surprised you about the frequency or types of data collected? How might this awareness influence your future digital behavior?

2. In writing your essay, did you encounter any tension between your personal views on security and privacy and what current laws allow? How did you navigate or reconcile that tension?

3. Do you have any ethical concerns about how your information is being collected and potentially used?
 
:::

## Data privacy laws in the United States of America

![Creepy condescending Wonka meme that says, "Please tell me more about your privacy policy."](www/images/05_wonka-privacy-policy.jpg){#fig-wonka-privacy-policy width=50%}

Prior to releasing any information (e.g., data and statistics), you must review the legal requirements. As you learned from the book and the questionnaire in the first week of class, the United States has no federal law regulating how personal data can be collected, stored, and used. A privacy policy does not equate to data privacy protections.

The laws governing data collection and dissemination are limited to specific federal agencies (e.g., U.S. Census Bureau) or specific data types (e.g., education).

### Federal privacy laws

::: callout-note
### Confidential Information Protection and Statistical Efficiency Act of 2002 

> CIPSEA establishes uniform confidentiality protections for information collected for statistical purposes by U.S. statistical agencies, and it allows some data sharing between the Bureau of Labor Statistics, Bureau of Economic Analysis, and Census Bureau. The agencies report to Office of Management and Budget on particular actions related to confidentiality and data sharing.
>
> The law gives the agencies standardized approaches to protecting information from respondents so that it will not be exposed in ways that lead to inappropriate or surprising identification of the respondent. By default the respondent's data is used for statistical purposes only. If the respondent gives informed consent, the data can be put to some other use.

From [Wiki](https://en.wikipedia.org/wiki/Confidential_Information_Protection_and_Statistical_Efficiency_Act).

> A reauthorization of CIPSEA in 2018-19 gave the statistical agencies more opportunities to use administrative data for statistical purposes, and required them to more deeply analyze risks to privacy and confidentiality of respondents.

Foundations for Evidence-Based Policymaking Act of 2018 is often referred to as the Evidence Act and updated CIPSEA.

::: 

::: callout-note
### Children's Online Privacy Act of 1998

> This Act protects children's privacy by giving parents tools to control what information is collected from their children online. The Act requires the Commission to promulgate regulations requiring operators of commercial websites and online services directed to children under 13 or knowingly collecting personal information from children under 13 to: (a) notify parents of their information practices; (b) obtain verifiable parental consent for the collection, use, or disclosure of children’s personal information; (c) let parents prevent further maintenance or use or future collection of their child’s personal information; (d) provide parents access to their child’s personal information; (e) not require a child to provide more personal information than is reasonably necessary to participate in an activity; and (f) maintain reasonable procedures to protect the confidentiality, security, and integrity of the personal information. In order to encourage active industry self-regulation, the Act also includes a "safe harbor" provision allowing industry groups and others to request Commission approval of self-regulatory guidelines to govern participating websites’ compliance with the Rule.

From the [Federal Trade Commission](https://www.ftc.gov/legal-library/browse/statutes/childrens-online-privacy-protection-act).
::: 

::: callout-note
### The Family Educational Rights and Privacy Act (FERPA) of 1974

> The FERPA is a federal law that affords parents the right to have access to their children’s education records, the right to seek to have the records amended, and the right to have some control over the disclosure of personally identifiable information from the education records. When a student turns 18 years old, or enters a postsecondary institution at any age, the rights under FERPA transfer from the parents to the student (“eligible student”). The FERPA statute is found at 20 U.S.C. § 1232g and the FERPA regulations are found at 34 CFR Part 99.

From the [U.S. Department of Education](https://studentprivacy.ed.gov/faq/what-ferpa)
::: 

![Kat Coyle, the designer of the pussy hat, wearing a pink knitted hat referred to as the pussy hat.](www/images/pussy-hat.jpeg){#fig-pussy-hat}

::: callout-note
### The Health Insurance Portability and Accountability Act (HIPAA) of 1996 

> The HIPAA Privacy Rule establishes national standards to protect individuals' medical records and other individually identifiable health information (collectively defined as “protected health information”) and applies to health plans, health care clearinghouses, and those health care providers that conduct certain health care transactions electronically. The Rule requires appropriate safeguards to protect the privacy of protected health information and sets limits and conditions on the uses and disclosures that may be made of such information without an individual’s authorization. The Rule also gives individuals rights over their protected health information, including rights to examine and obtain a copy of their health records, to direct a covered entity to transmit to a third party an electronic copy of their protected health information in an electronic health record, and to request corrections.

From the [The HIPAA Privacy Rule from U.S. Department of Human and Health Services](https://www.hhs.gov/hipaa/for-professionals/privacy/index.html).

Effective as of June 25, 2024: [HIPAA Privacy Rule To Support Reproductive Health Care Privacy](https://www.federalregister.gov/documents/2024/04/26/2024-08503/hipaa-privacy-rule-to-support-reproductive-health-care-privacy) will "...strengthen privacy protections for medical records and health information for women."

::: 

::: callout-note
### Title 13, U.S. Code of 1953

> The Census Bureau is bound by Title 13 of the United States Code. These laws not only provide authority for the work we do, but also provide strong protection for the information we collect from individuals and businesses.
>
> Title 13 provides the following protections to individuals and businesses:
>
> - Private information is never published. It is against the law to disclose or publish any private information that identifies an individual or business such, including names, addresses (including GPS coordinates), Social Security Numbers, and telephone numbers.
> - The Census Bureau collects information to produce statistics. Personal information cannot be used against respondents by any government agency or court.
> - Census Bureau employees are sworn to protect confidentiality. People sworn to uphold Title 13 are legally required to maintain the confidentiality of your data. Every person with access to your data is sworn for life to protect your information and understands that the penalties for violating this law are applicable for a lifetime.
> - Violating the law is a serious federal crime. Anyone who violates this law will face severe penalties, including a federal prison sentence of up to five years, a fine of up to $250,000, or both.

From the [U.S. Census Bureau](https://www.census.gov/history/www/reference/privacy_confidentiality/title_13_us_code.html)
::: 

::: callout-note
### Title 26, U.S. Code of 1939

> The Internal Revenue Code (IRC) is the body of law that codifies all federal tax laws, including income, estate, gift, excise, alcohol, tobacco, and employment taxes. U.S. tax laws began to be codified in 1874, but there was no central, comprehensive source for them at that time. The IRC was originally compiled in 1939 and overhauled in 1954 and 1986. This code is the definitive source of all tax laws in the United States and has the force of law in and of itself.
> 
> These laws constitute Title 26 of the U.S. Code (26 U.S.C.A. § 1 et seq. [1986]) and are implemented by the Internal Revenue Service (IRS) through its Treasury Regulations and Revenue Rulings.
>
> Congress made major statutory changes to Title 26 in 1939, 1954, and 1986. Because of the extensive revisions made in the Tax Reform Act of 1986, Title 26 is now known as the Internal Revenue Code of 1986 (Pub. L. No. 99-514, § 2, 100 Stat. 2095 [Oct. 22, 1986]).

From the [U.S. Census Bureau](https://www.census.gov/history/www/reference/privacy_confidentiality/title_26_us_code_1.html)

:::

### State privacy laws

![IAPP's U.S. State Privacy Legislation Tracker Image, last updated July 7, 2025 .](www/images/05_2025-state-law.png){#fig-iapp-state fig-align="center" width=100%}

The International Association of Privacy Professionals (IAPP)[^IAPP] has a great
[U.S. State Privacy Legislation Tracker](https://iapp.org/resources/article/us-state-privacy-legislation-tracker/).

>State-level momentum for comprehensive privacy bills is at an all-time high. The IAPP Westin Research Center actively tracks the proposed and enacted comprehensive privacy bills from across the U.S. to help our members stay informed of the changing state privacy landscape. This information is compiled into a chart, map and a directory with information specific to states with enacted laws. The IAPP additionally hosts a US State Privacy topic page, which regularly updates with the latest state privacy news and resources, and a US State AI Governance Legislation Tracker, which tracks US state cross-sectoral laws with direct application to the use of AI systems in the private sector.

[^IAPP]: The International Association of Privacy Professionals is a nonprofit, non-advocacy membership association founded in 2000.


![Candalf You Shall Not Pass Meme that says, "This too shall pass."](www/images/05_this-too-shall-pass.jpg){#fig-shall-pass}

### California Privacy Act of 2018

From the [State of California Department of Justice](https://oag.ca.gov/privacy/ccpa):

The California Consumer Privacy Act (CCPA) gives consumers more control over the personal information that businesses collect about them, with accompanying regulations that provide guidance on how to implement the law, which includes:

- The right to know about the personal information a business collects about them and how it is used and shared;
- The right to delete personal information collected from them (with some exceptions);
- The right to opt-out of the sale or sharing of their personal information; and
- The right to non-discrimination for exercising their CCPA rights.

In November 2020, California voters approved Proposition 24, the California Privacy Rights Act, which amended the CCPA and added additional privacy protections that took effect on January 1, 2023. As of this date, consumers have new rights in addition to those above, such as:

- The right to correct inaccurate personal information that a business has about them; and
- The right to limit the use and disclosure of sensitive personal information collected about them.

Businesses subject to the CCPA have several responsibilities, including responding to consumer requests to exercise these rights and providing consumers with certain notices explaining their privacy practices. The CCPA applies to many businesses, including data brokers.

::: callout
#### [`r paste("Class Activity", exercise_number)`]{style="color:#1696d2;"}
```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

1. If you could change or update one data privacy law (state or federal), what would it be and why?

2. What types of personal data do you think are most in need of stronger legal protections today?

:::

<!-- ### General Data Protection Regulation -->

<!-- The General Data Protection Regulation or GDPR, is a set of European Union data privacy laws and protections. The 99 articles of GDPR covers a range of topics from what rights does a person have for their personal data to how companies are penalized for violating the law. -->

<!-- #### Key aspects of GDPR -->

<!-- - The data containing identifiers are key for GDPR to consider the information as personal data. -->
<!-- - GDPR separates personal data into two categories: general personal data and sensitive personal -->
<!-- data.  -->
<!-- - Sensitive data is racial or ethnic origin, political opinions, religious or philosophical beliefs, trade union membership, the processing of genetic data and biometric data for the purpose of uniquely identifying a natural person, and sex life or sexual orientation. -->

<!-- Revealing this type of personal data is prohibited. GDPR precludes the processing of sensitive personal information and only for special circumstances can a company process such data. -->

<!-- GDPR defines those responsible for protecting personal data, as: -->

<!-- - the *controller* one responsible for deciding the purpose and processing of the personal data. -->
<!-- - the *processor* acts on behalf of the controller while processing personal data. -->

<!-- #### Seven principles for proceessing personal data -->

<!-- GDPR states the seven principles for processing personal data. These seven principles are called: -->

<!-- 1. Lawfulness, Fairness, and Transparency; -->
<!-- 2. Purpose Limitation; -->
<!-- 3. Data Minimization; -->
<!-- 4. Accuracy; -->
<!-- 5. Storage Limitation; -->
<!-- 6. Integrity and Confidentiality; and -->
<!-- 7. Accountability. -->

<!-- #### The right to privacy and the right to be forgotten -->

<!-- GDPR monitors what organizations do with people’s personal data, empowers people to control how their personal data are being collected and used by others, and forces organizations to justify anything they do with people’s personal data. -->

<!-- organizations to use personal data if at least one of the six lawful bases apply. These bases are referred as: -->

<!-- 1. consent, which must be “...freely given, specific, informed and unambiguous indication of the data subject’s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.” -->

<!-- 2. performance of a contract, where a processor requires someone’s personal data to complete specific steps outlined in a contract or prior to entering into a contract. -->

<!-- 3. legal obligation, when a company must use personal data to comply with other laws. -->

<!-- 4. vital interest, where a business can access personal data to protect the person’s life and others. -->

<!-- 5. public interest, such that an organization might perform certain tasks for people’s public interest or to complete official functions. -->

<!-- 6. legitimate interest, where these interests may include situations where someone would expect an organization to collect personal data, use the data in ways that are low-risk, or make a compelling argument on the benefits outweighing the risks. -->

## Identifying the technical and policy solutions

::: {.callout-important}
Technical and policy approaches in combination are necessary for effective management of disclosure risks.
::: 

Agencies and private companies can use a variety of technical and policy approaches, possibly in combination, to achieve disclosure risk/usefulness trade-offs that are acceptable to various stakeholders. Confronted with myriad options, how can an entity arrive at an ideal approach?

Answers to this question are necessarily context specific. However, some decision points are common to many data dissemination scenarios.

![Model decision matrix of disclosure-protection strategies given potential disclosure harms and data usefulness from @national2024toward.](www/images/05_tech-policy-chart.png){#fig-tech-policy fig-align="center" width=100%}

> In the interior cells, “Access” refers to approaches that could be particularly effective for managing disclosure risk/usefulness trade-offs. “RD” refers to restricted data, which includes policy approaches like secure data enclaves, licensing and other data-use agreements, penalties and incentives for responsible use, and query auditing. “DL” refers to disclosure limitation, which includes techniques like formal privacy, secure multiparty computation, and synthetic data approaches. “Tiers” refers to tiered access, which may include validation/verification of results from redacted data and access to confidential data under policy restrictions. “Privacy” refers to a general characterization of the desired privacy protections/guarantees on blended data products.

### Current statistical data privacy workflow

The current standard workflow for statistical data privacy applications includes three main components: (1) data, (2) privacy practitioner(s), and (3) decisionmaker(s). At the start, privacy practitioners review the data (at the least the data schema[^schema]) and talk with the decisionmakers. From there, the privacy practitioners determine the privacy loss and utility framework of definitions, with possible input from the data and the decisionmakers. Decisionmakers then choose the threshold levels of privacy loss and utility, which depend on the legal or societal context and any practical constraints.

[^schema]: For example, the variables contained in the data, the number of observations, or the range of values for each variable.

From here, the privacy practitioner selects the statistical data privacy method that meets the desired privacy loss and utility definitions and thresholds. This decision may also use information from the data. Finally, after all the prior decisions are made, the privacy practitioner implements the statistical data privacy method on the data. In some instances, feedback loops exist.

::: callout
#### [`r paste("Class Activity", exercise_number)`]{style="color:#1696d2;"}
```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

Based on what we've learned so far from in the class, what are the issues with this workflow?

:::

![The Rock saying, "This doesn't work."](www/images/05_doesnt-work.gif){#fig-doesnt-work}

### Aspirational equitable statistical data privacy workflow

As discussed in the class activity, there are obvious issues with the previous workflow. This is why @bowen2023no proposed an aspirational workflow. It is called aspirational because we do not expect every step to always be possible in a real-world implementation. That being said, considering these steps can help make equity part of the process from beginning to end. Below, we describe the six steps of the process in greater detail.

![A visual representation of the aspirational equitable statistical data privacy workflow from @bowen2023no.](www/images/05_aspirational-workflow.png){#fig-aspirational fig-align="center" width=100%}

1. Identify groups in the data 

2. Identify appropriate representatives for the groups

3. Determine privacy and utility definitions and preferences with representatives and decisionmakers

    a. Define statistical utility and privacy loss
    b. Choose group-level preferences for statistical utility and privacy loss thresholds

4. Communicate constraints to representatives and decisionmakers

    a. Use metrics and visuals for group-level tradeoff curves
    b. Explain technical shortcomings related to group size

5. Choose a statistical data privacy implementation that best satisfies definitions, preferences, and constraints

6. Publish each step of the process

Every step in the process has limitations, so the entire process must be transparent. Because each of the prior steps is aspirational and may require significant growth in the field, transparency is one of the most immediate actions that practitioners can take. The process could be reported in public presentations, documentations, or regular meetings [@murray2016data; @cohen2022guide]. This transparency allows outside groups and entities to critique the decisions, offer input during the process, and understand what decisions were made. With feedback, each step of the process can be refined, such as selecting group representatives, choosing preferences, or navigating technical shortcomings.

### A Framework for Managing Disclosure Risks in Blended Data

> The framework begins with a simple but critical question for agencies considering a data-blending project: What do we want to accomplish with blended data, and why?
[@national2024toward]

  1. Determine auspice and purpose of the blended data project.
  
      a. What are the anticipated final products of data blending?
      b. What are potential downstream uses of blended data?
      c. What are potential considerations for disclosure risks and harms, and data usefulness?
    
  2. Determine ingredient data files.
  
      a. What data sources are available to accomplish blending, and what are the interests of data holders?
      b. What steps can be taken to reduce disclosure risks and enhance usefulness when compiling ingredient files?
    
  3. Obtain access to ingredient data files.
  
      a. What are the disclosure risks associated with procuring ingredient data?
      b. What are the disclosure risk/usefulness trade-offs in the plan for accessing ingredient files?
    
  4. Blend ingredient data files.
  
      a. When blending requires linking records from ingredient files, what linkage strategies can be used?
      b. Are resultant blended data sufficiently useful to meet the blending objective?
    
  5. Select approaches that meet the end objective of data blending.
  
      a. What are the best-available scientific methods for disclosure limitation to accomplish the blended data objective, and are sufficient resources available to implement those methods?
      b. How can stakeholders be engaged in the decision-making process?
      c. What is the mitigation plan for confidentiality breaches?
    
  6. Develop and execute a maintenance plan.
  
      a. How will agencies track data provenance and update files when beneficial?
      b. What is the decision-making process for continuing access to or sunsetting the blended data product, and how do participating agencies contribute to those decisions?
      c. How will agencies communicate decisions about disclosure management policies with stakeholders?

## College Scorecard Example

The following description is from @national2024toward, which I helped write and lead this part of the report. The College Score card is a great example of how to consider the technical and policy solutions to release confidential data for the public good.

The [College Scorecard](https://collegescorecard.ed.gov/) involves an approach to blending aggregated data from the Statistics of Income (SOI) Program of IRS with College Scorecard data (U.S. Department of Education) from the Department of Education (ED).

### Determine auspice and purpose

The College Scorecard is intended to help future college students and their families search for and compare colleges by field of study, costs, admissions, economic outcomes, and other statistics. Since 2016, ED has provided the College Scorecard as a web-based search tool (i.e., data product) that users can query repeatedly; public-use microdata files are not required. The unit of analysis is the college, and the data are summaries of the features of the college and its students. ED desired to add earning metrics to the College Scorecard. Such blended data could offer the public additional insights into student outcomes at various educational institutions; however, tax records contain very sensitive information and are protected by Title 26, requiring rigorous privacy guarantees.

From @fig-tech-policy, we might characterize the College Scorecard containing earnings information as being at the intersection of “Modest Impact” usefulness and “Significant and Lasting” disclosure risks.

### Identify ingredient data files

To create the blended data, ED requires earnings data on college graduates. ED therefore turned to SOI for such data. Education data are subject to FERPA, whereas SOI data are subject to Title 26 rules. This means certain fields may be released as is (e.g., institution), but others require confidentiality protection (e.g., earnings data at the field-of-study or institution level). There is no federal source of average earnings per college, requiring the agencies to blend data at the individual student level. Therefore, the ingredient data file from ED includes recipients of federal student aid,[^aid] and the ingredient data file from IRS includes individuals’ earnings. These files need to be linked to accomplish the blending.

[^aid]: It is important to note that not all students who are in need receive federal student aid, so
there may be some bias in generating the College Scorecard.

### Obtain ingredient data files

Because SOI data derive from tax records, individual students’ earnings cannot be shared outside the agency without special agreements. Thus, SOI needs to blend its data with the data supplied by ED. However, Internal Revenue Code (IRC) § 6108(b) permits SOI to release aggregated-level statistics to ED. The IRC authorizes the Secretary of the Treasury to make special statistical studies and compilations involving tax return information as defined in IRC § 6103(b)(2).

### Blend ingredient data files

ED provides student-level records—that is, recipients of federal student aid—to SOI to conduct the matching at IRS. The data-sharing arrangement allows SOI to match student information to IRS administrative tax records, which are W-2 (i.e., wage and tax statement) and 1040-SE (i.e., net earnings from self-employment) data. Matching is done on social security numbers.

### Select disclosure-protection approaches
For the first 3 years of the product, the College Scorecard used classical disclosure approaches (e.g., data suppression, rounding, aggregation, and top-coding) to protect blended data. During this time, ED and SOI continued to reassess the disclosure risk/ usefulness trade-offs of their dissemination strategy. In 2020, ED and SOI wanted to produce a second data file with greater granularity than available at the institution level—one that contained information at the credential and field-of-study level. But SOI became concerned about potential complementary disclosures in creating two datasets from the same ingredient data, as their disclosure approaches did not account for multiple data releases.

SOI also determined that using the classical disclosure approaches to release the second, more granular data file would suppress and alter substantial amounts of blended data, thereby degrading the potential usefulness of the data product.

In an update of disclosure-protection processes, SOI decided to use SafeTables, a software package developed by [Tumult Labs](ttps://www.tmlt.io/) that produces statistics from a formally private, noise-addition algorithm. SOI saw three benefits of using a formally private framework. First was the potential ability to quantify privacy risks when releasing datasets. Second was establishing a privacy-loss budget and a mechanism to examine specific points on the disclosure risk/usefulness trade-off. SOI’s final reason was composition: SOI could potentially track total privacy loss across multiple releases of outputs.[^safetables]

[^safetables]: Additionally, the use of SafeTables allowed suppression of fewer table cells than would have been required had simple cell suppression been used.

Although formal privacy methods are used, the approach of ED and SOI to releasing College Scorecard data is a hybrid between formal privacy and classical disclosure approaches. ED and SOI suppress outputs from SafeTables that differ excessively (per their definition) from the confidential data values to enhance usefulness of published data. Therefore, the end product is not technically formally private; the decision to suppress is based on confidential data values, which violates the formal privacy guarantee. SOI accepted the implied higher disclosure risks, suppressing the noisiest results, because the program deemed it important that students have accurate statistics upon which to make potentially life-changing decisions.

::: callout-tip
### Definition of Formal Privacy

Although the privacy community has not fully agreed on a common definition, the U.S. Census Bureau[^05_differential-privacy-1] defines **formal privacy** as a subset of SDC methods that give "formal and quantifiable guarantees on inference disclosure risk and known algorithmic mechanisms for releasing data that satisfy these guarantees."

[^05_differential-privacy-1]: "Consistency of data products and formally private methods for the 2020 census," U.S. Census Bureau, p. 43, https://irp.fas.org/agency/dod/jason/census-privacy.pdf

In general, formally private methods have the following features [@bowen2021philosophy]:

-   Ability to quantify and adjust the privacy-utility trade-off, typically through parameters.
-   Ability to rigorously and mathematically prove the maximum privacy loss that can result from the release of information.
-   Formal privacy definitions also allow one to *compose* multiple statistics. In other words, a data curator can compute the total privacy loss from multiple individual information releases.
:::

To help determine the privacy-loss budget prior to suppressing values, SOI and ED used an evaluation tool called CSExplorer. Also developed by Tumult Labs, this tool allows the client (i.e., SOI and ED) to review and evaluate the disclosure risk/usefulness trade-offs of the output statistics for various privacy-loss budgets. SOI first used the tool to examine the effects of several privacy-loss budgets on various usefulness metrics. Once SOI identified a set of appropriate privacy-loss budgets, ED had access to a limited version of CSExplorer that was in the range of SOI’s selected privacy-loss budgets. ED then could explore the statistical outputs based on those privacy-loss budgets to examine and decide upon which suppression thresholds to use and other specifications for the final data product release.

This tool allowed ED and SOI to have thoughtful, in-depth discussions about trade-offs between privacy and usefulness.

### Develop and execute a maintenance plan

The College Scorecard does not publicize its disclosure-protection processes on its website, and it keeps values of privacy parameters internal. Providing such information along with source code for the disclosure-protection algorithms would improve transparency. @national2024toward is also unaware of mitigation strategies for potential breaches.

We conclude this case study by mentioning an application with similar goals as the College Scorecard, namely the Census Bureau’s Post-Secondary Employment Outcomes (PSEO) dataset. This data product provides tables of earnings and employment outcomes of graduates from postsecondary institutions [@foote2019releasing], although it does not involve IRS data. The PSEO also applies a formal privacy method to publish the tables.

::: callout
#### [`r paste("Class Activity", exercise_number)`]{style="color:#1696d2;"}
```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

After reviewing the two proposed workflows and the accompanying example, answer the following questions:

- Do these workflows effectively address the concerns and issues you had with the current/original workflow?
- What are some possible challenges in executing these new workflows?
- Are there any additional improvements or adjustments you would suggest for these new workflows?
:::

![Gene Wilder saying, "It. Could. Work."](www/images/could-work.gif){#fig-could-work}
