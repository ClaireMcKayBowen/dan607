---
title: "Data Storage"
format: 
  html:
    toc: true
    code-line-numbers: true
editor_options: 
  chunk_output_type: console
---


```{r}
#| echo: false

exercise_number <- 1

```

## This part of the course is still under construction! Proceed with caution!

![Webpage under construction!](www/images/under-construction.jpg){#fig-under-construction fig-align="center" width=80%}


For this week, we learn about how we should properly store confidential or sensitive data and how to share or transfer that data safely.

## Quick recap on week 2

We should think about the interactions of this ecosystem the data life cycle in the context of security, privacy, ethics, and equity.

### Definitions

![Key stakeholders in the data ecosystem](www/images/stakeholders.png){#fig-stakeholders fig-align="center"}

::: callout-tip
### Data Security

**Data Security** is the "...science of methods of protecting computer data and communication systems that apply various types of controls such as cryptography, access control, information flow paths and inference control, including backup and recover" [@denning1982cryptography].
:::

::: callout-tip
### Data Privacy

**Data Privacy** is the ability "to determine what information about ourselves we will share with others" [@fellegi1972question]. Data privacy is a broad topic, which includes data security, encryption, access to data, etc. We will not be covering privacy breaches from unauthorized access to a database (e.g., hackers).
:::

::: callout-tip
## Confidentiality

**Confidentiality** is "the agreement, explicit or implicit, between data subject and data collector regarding the extent to which access by others to personal information is allowed" [@fienberg2018statistical].
:::

::: callout-tip
## Ethics

**Data ethics** is the "...systemizing, defending, and recommending concepts of right and wrong conduct in relation to data, in particular personal data" [@kitchin2014data].
:::

::: callout-tip
## Equity

**Data Equity** is data representation, access, process, outcomes, and more
:::

### Data types

We learned about the three principal types of qualitative and quantitative data:

1.	*Primary:* Any data directly collected by an entity. 

2.	*Secondary:* Any data collected by another organization that a stakeholder uses for analysis.

3.	*Administrative:* Any data collected by governments or other organizations, as part of their management and operation of a program or service, that provide information on registrations, transactions, and other regular tasks.

### Data collections challenges

We've discussed several challenges surrounding security, privacy, ethics, and equity in data collection. Specifically, we've highlighted the importance of:

- Deciding whether to collect the data or not.
- Defining groups and other variables that impacts our communities.
- Recognizing that a lack of data results in a lack of action.

## Why is data storage so important?

> Archives and domain repositories that preserve and disseminate social and behavioral data perform a critical service to the scholarly community and to society at large by ensuring that these culturally significant materials are accessible in perpetuity. The success of the archiving endeavor, however, ultimately depends on researchers’ willingness to deposit their data and documentation for others to use.

From Inter-university Consortium for Political and Social Research [@icpsr], one of the world's largest data archives of social science data for research and education.

### More definitions

In addition, there are many versions of the data we should define.

::: {.callout-tip}
## Original dataset:

**Original dataset** is the uncleaned, unprotected version of the data.

For example, raw 2020 Decennial Census microdata, which are never publicly released.
::: 

::: {.callout-tip}
## Confidential or gold standard dataset

**Confidential or gold standard dataset** is the cleaned version (meaning edited for inaccuracies or inconsistencies) of the data; often referred to as the gold standard or actual data for analysis. 

For example, the Census Edited File that is the final confidential data for the 2020 Census. This dataset is never publicly released but may be made available to others who are sworn to protect confidentiality (i.e., Special Sworn Status) and who are provided access in a secure environment, such as a Federal Statistical Research Data Center.
::: 

::: {.callout-tip}
## Public dataset or statistics

**Public dataset** is the publicly released version of the confidential data.

For example, the US Census Bureau's public tables and datasets or the Bureau of Labor Statistics reporting the unemployment rate statistics.
::: 

Different levels of security and privacy are needed for different versions of the data. We will mostly focus on the confidential and public versions of the data. However, note that the original or raw data must also be securely stored and properly documented for future reference.

### Data cleaning

After data collection, the data curator will often need to clean the raw data.

the data are often messy, inconsistent, and contain missing values, among other issues. Cleaning the data takes up the majority of the time in any data project, from start (data collection) to finish (data dissemination and/or destruction). This important task converts the data into a usable form for others.

"Tidy data" tends to have the following features:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a dataframe.

![Artwork by Allison Horst.](www/images/what_is_tidydata.jpeg){#fig-tidydata fig-align="center"}

Given the conceptual focus of the course, we will not be doing any exercises involving data cleaning. Instead, we will cover some of the things you should consider if you are ever tasked with data cleaning.

Everyone has their own way of cleaning data and you will find several sources with different steps to clean data. All these different sources have roughly the same set of steps or aspects to check for when data cleaning, which we will discuss.

::: {.callout-important}
## Create backups!
Always make a backup copy of the original data prior to cleaning it just in case something happens!
::: 

#### Check for data quality issues
Common issues include duplicate observations or records, spelling errors, incorrect numbers and number signs, and inconsistencies where the sum doesn't equal the total (e.g., the number of people in each county of Massachusetts should equal the state total).

::: {.callout-caution}
## Careful about outliers
Some people suggest removing outliers, but you should be very careful about this practice. The outlier may be a true signal of something happening in your data. Proceed with caution when deciding to remove observations or values in the data if they are outliers.
::: 

#### Standardize the data
This involves changing text case, removing spaces and non-printing characters from text, fixing dates and times (a HUGE issue!), and ensuring consistent units across variables.

#### Define how to handle missing data
Missing data can significantly impact downstream uses of the data. Clearly document how you handled missing data. Options include removing the records or imputing the missing values using a statistical technique.

#### Validate the data cleaning (Quality assurance)
Always validate or conduct a quality assurance procedure to ensure you have taken the proper steps to clean the data.


::: callout
#### [`r paste("Class Activity", exercise_number)`]{style="color:#1696d2;"}

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

Suppose you are tasked to clean data collected about 

:::


## Data security basics

For all data, you should indicate how and where you will store copies of your research files to ensure their safety, as well as how many copies you will keep and how you will synchronize them (You do NOT want multiple versions of one data floating around!). One best practice for protecting data is to store multiple copies in multiple locations. How and where you store the data depends on the contents of the data, privacy laws in place, your workplace practices, any contractual agreements, and other factors. We will touch on a few of these issues, but know there are many others, and you should always consult with the data security officer of your institution or workplace.

### Virtual data storage

Most universities, privacy companies, government agencies, and other places will maintain confidential disks separate from nonconfidential disks for their system servers. Any confidential data stored on disks should have some sort of encryption system, such as Pretty Good Privacy[^PGP], at the folder level. The confidential disks are backed up via a disk-to-disk on-premises system that securely deletes all backups after six months.

[^PGP]: "Pretty Good Privacy is an encryption program that provides cryptographic privacy and authentication for data communication." from https://en.wikipedia.org/wiki/Pretty_Good_Privacy

If someone uses an account or a computer with access to confidential data, they must not leave the session unattended, must log out at the end of the session, and must lock up any storage media that hold confidential data.

Other good data security practices include:

- limiting access to confidential data to authorized users, such as through a secure login
- multiple levels of encryption
- prevention of individuals making copies of the confidential data to nonconfidential disks (or copying the data at all)
- virus and intruder protections

### Physical data storage

If confidential data are stored on a PC, the data should be encrypted or stored on removable storage media that is secured in a locked cabinet when not in use.

All storage media (e.g., CDs, internal and external hard drives, flash drives) that hold confidential data must be explicitly labeled “confidential.” The project security officer or project manager must maintain a log for each piece of confidential storage media and record dates for the following: 

- receipt of the item from external source 
- creation of the item at the institution 
- destruction of the item 
- transfer of the item to someone else's responsibility (even within the institution) 

::: {.callout-important}
## Security and privacy trainings and audits
Frequently, the workplace and/or entities allowing data use must participate in annual training sessions on the appropriate handling of confidential data. Additionally, they conduct semiannual reviews of confidential logs to ensure that all confidential media is properly accounted for.
::: 

## Importance of meta data

> Metadata are essential for maximizing the usefulness of data. Because it is often impossible for secondary researchers to ask questions of the original data producers, metadata are the de facto form of communication between them. Comprehensive metadata standardizes how the data are described, enables a deeper comprehension of a dataset, facilitates data searches by variables, and offers a variety of display options on the Web.

::: callout-tip
## Meta data

**Metadata** is "information about the data collections that help others discover, understand, and use them."

From ICPSR's webpage on [Metadata](https://www.icpsr.umich.edu/web/pages/datamanagement/lifecycle/metadata.html).
:::

The lack of metadata across public datasets from local, state, territory, tribal, and federal government agencies is one reason why the The Open, Public, Electronic and Necessary (OPEN) Government Data Act was included as a section in the [2018 Foundations for Evidence-Based Policymaking Act](https://www.datacoalition.org/resources/Documents/BILLS-115HR4174ih-FEBP.pdf), a bipartisan U.S. law pushing the federal government to modernize the data infrastructure, such as data management, statistical efficiency, and more.

To help conceptually understand what kind of information should be a part of meta data, we will follow the FAIR guiding principles. FAIR aims to make data findable, accessible, interoperable, and reusable [@wilkinson2016fair]. I will use the [Urban Data Catalog](https://datacatalog.urban.org/) and [ICPSR](https://www.icpsr.umich.edu/web/pages/ICPSR/index.html) for examples.

::: {.callout-tip}
## Document, Discover, and Interoperate
[Document, Discover, and Interoperate](https://ddialliance.org/) (DDI) is a standard that follows the FAIR principles and many major data repositories like ICPSR uses DDI!

::: 

### Findable

1. Data and metadata are assigned a globally unique, eternally persistent identifier.
2. Data are described with rich metadata.
3. Meta)data are registered or indexed in a searchable resource.
4. Metadata specify the data identifier.

### Accessible

1. (Meta)data are retrievable by their identifier using a standardized communications protocol.
    - The protocol is open, free, and universally implementable.
    - The protocol allows for an authentication and authorization procedure, where necessary.
2. Metadata are accessible, even when data are no longer available.

### Interoperable

1. (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.
2. (Meta)data use vocabularies that follow FAIR principles.
3. (Meta)data include qualified references to other (meta)data.

### Reusable

1. Meta(data) have a plurality of accurate and relevant attributes.
    - (Meta)data are released with a clear and accessible data usage license.
    - (Meta)data are associated with their provenance.
    - (Meta)data meet domain-relevant community standards.
  
::: callout
#### [`r paste("Class Activity", exercise_number)`]{style="color:#1696d2;"}

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

TEXT

:::





