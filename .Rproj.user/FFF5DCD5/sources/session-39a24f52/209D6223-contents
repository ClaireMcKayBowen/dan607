---
title: "Utility and Disclosure Risk Metrics"
format:
  html:
    toc: true
    code-line-numbers: true
editor_options:
  chunk_output_type: console
execute:
  warning: false
  message: false
bibliography: references.bib
---

```{r setup4}
#| echo: false
library(tidyverse)
library(tidysynthesis)
library(syntheval)
library(palmerpenguins)
library(gt)
library(urbnthemes)
library(patchwork)
library(tidymodels)

# set Urban Institute data visualization styles
set_urbn_defaults(base_size = 12)

# set a seed so pseudo-random processes are reproducible
set.seed(20220301)
```

```{r echo = FALSE}
# drop NA values and convert variable types
penguins <- penguins %>%
  #select(-year) %>%
  filter(complete.cases(.)) %>%
  mutate(
    flipper_length_mm = as.numeric(flipper_length_mm),
    body_mass_g = as.numeric(body_mass_g)
  )

```

## Assessing utility

Generally there are 2 ways to measure utility of the data:

-   General (or global) utility metrics; and
-   Specific utility metrics.

<br>

### General utility metrics

::: callout-tip
### General utility

**General utility**, sometimes called global utility, measures the univariate and multivariate distributional similarity between the confidential data and the public data (e.g., sample means, sample variances, and the variance-covariance matrix).
:::

General utility metrics are useful because they provide a sense of how "fit for use" synthetic data is for analysis without making assumptions about the uses of the synthetic data.

<br>

#### Univariate general utility

Some univariate general utility measures could include comparisons of:

-   **Categorical variables:** frequencies, relative frequencies

-   **Numeric variables** means, standard deviations, skewness, kurtosis (i.e., first four moments), percentiles, and number of zero/non-zero values

<br>

#### Bivariate general utility

::: callout-tip
##### Correlation fit

**Correlation fit** measures how well the synthesizer recreates the linear relationships between variables in the confidential dataset.
:::

To calculate correlation fit:

-   Create correlation matrices for the synthetic data and confidential data. Then measure differences across synthetic and actual data. Those differences are often summarized across all variables using [L1](https://en.wikipedia.org/wiki/Taxicab_geometry) or [L2](https://en.wikipedia.org/wiki/Euclidean_distance) distance.

![Correlation Difference](www/images/puf-correlation-fit.png){#fig-corrdiff}

<br> <br> <br>

@fig-corrdiff shows the creation of a difference matrix. Let's summarize the difference matrix using mean absolute error. This will give us a sense of how off the average correlation will be in the synthetic data compared to the confidential data.

$$MAE_{dist} = \frac{1}{n}\sum_{i = 1}^n |dist|$$

$$MAE_{dist} = \frac{1}{6} \left(|-0.15| + |0.01| + |0.1| + |-0.15| + |0.15| + |0.02|\right) \approx `r mean(abs(c(-0.15, 0.01, 0.1, -0.15, 0.15, 0.02)))`$$

Advanced measures like *relative mutual information* can be used to measure the relationships between categorical variables.

<br>

#### Multivariate general utility (discriminant-based metrics)

::: callout-tip
##### Discriminant based methods

**Discriminant based methods** measure how well a predictive model can distinguish (i.e., discriminate) between records from the confidential and synthetic data.
:::

-   The confidential data and synthetic data should theoretically be drawn from the same super population.

-   The basic idea is to combine (stack) the confidential data and synthetic data and see how well a predictive model distinguish (i.e., discriminate) between synthetic observations and confidential observations.

-   An inability to distinguish between the records suggests a good synthesis.

-   It is possible to use logistic regression for the predictive modeling, but decision trees, random forests, and boosted trees are more common. (We recommend, to the degree possible, using more using more sophisticated models as well as machine learning best-practices like feature engineering and hyperparameter tuning because these practices will more effectively discriminate between classes.)

-   @fig-discriminant shows three discriminant based metrics calculated on a good synthesis and a poor synthesis.

::: {#fig-discriminant layout-nrow="2"}
![Good Synthesis](www/images/same-population-disc-metrics.png){width="518"}

![Poor Synthesis](www/images/different-population-disc-metrics.png){width="518"}

A comparison of discriminant metrics on a good synthesis and a poor synthesis
:::

<br> <br> <br>

##### Calculating Discriminant Metrics

pMSE ratio, SPECKS, and AUC all require calculating propensity scores (i.e., the probability that a particular data point belongs to the confidential data) and start with the same first two steps.

1)  *Combine the synthetic and confidential data. Add an indicator variable with 0 for the confidential data and 1 for the synthetic data*

```{r, echo = FALSE}
  
set.seed(1297)
      
x = penguins %>%
  select(species, bill_length_mm, sex) %>% 
  sample_n(2) %>% 
  add_row(.before = 2) %>% 
  mutate(ind = c(0, NA, 1))
      
x %>% 
  gt() %>% 
  fmt_missing(columns = everything(), 
              missing_text = "...") %>% 
  tab_style(cell_fill(color = palette_urbn_main["cyan"], alpha = 0.3), 
                    locations = cells_body(columns = ind))
```

2)  *Calculate propensity scores (i.e., probabilities for group membership) for whether a given row belong to the synthetic dataset.*

```{r, echo = FALSE}
set.seed(1297)
    
    
x %>% 
  mutate(prop_score = c(0.32, NA, 0.64)) %>% 
  gt() %>% 
  fmt_missing(columns = everything(), 
              missing_text = "...") %>% 
  tab_style(cell_fill(color = palette_urbn_main["cyan"], alpha = 0.3), 
                    locations = cells_body(columns = prop_score))
```

<br>

::: panel-tabset
##### ROC Curves/AUC

**Receiver Operating Characteristic (ROC) curves** show the trade off between false positives and true positives. Area under the curve (AUC) is a single number summary of the ROC curve.

AUC is a common tool for evaluating classification models. High values for AUC suggest the model can distinguish between confidential and synthetic observations, indicating a poor synthesis.

After generating propensity scores (i.e., steps 1 and 2 from above), calculate the ROC curve and corresponding AUC for the model you fit.

![](www/images/roc-curve.png){width="572"}

-   In our context, ***High AUC*** = good at discriminating = ***poor synthesis***.

-   In the best case, AUC = 0.5 because that means the discriminator is no better than a random guess

##### pMSE

**pMSE**: Calculates the average Mean Squared Error (MSE) between the propensity scores and the expected probabilities:

-   Proposed by @woo2009global and enhanced by @snoke_raab_nowok_dibben_slavkovic_2018

-   After doing steps 1) and 2) above:

    3)  *Calculate expected probability, i.e., the share of synthetic data in the combined data.* In the cases where the synthetic and confidential datasets are of equal size, this will always be 0.5.

        ```{r, echo = FALSE}
        set.seed(1297)
            
            
                                x %>% 
          mutate(prop_score = c(0.32, NA, 0.64),
                 exp_prob = c(0.5, NA, 0.5)) %>% 
          gt() %>% 
          fmt_missing(columns = everything(), 
                      missing_text = "...") %>% 
          tab_style(cell_fill(color = palette_urbn_main["cyan"], alpha = 0.3), 
                    locations = cells_body(columns = exp_prob))
        ```

    4)  *Calculate pMSE, which is mean squared difference between the propensity scores and expected probabilities.*

    $$pMSE = \frac{(0.32 - 0.5)^2 + ... + (0.64-0.5)^2}{N} $$

-   Often people use the pMSE ratio, which is the average pMSE score across all records, divided by the null model [@snoke2018general].

-   The null model is the the expected value of the pMSE score under the best case scenario when the model used to generate the data reflects the confidential data perfectly.

-   pMSE ratio = 1 means that your synthetic data and confidential data are indistinguishable, although values this low are almost never achieved.

##### SPECKS

**SPECKS**: **S**ynthetic data generation; **P**ropensity score matching; **E**mpirical **C**omparison via the **K**olmogorov-**S**mirnov distance.

After generating propensity scores (i.e., steps 1 and 2 from above), you:

3)  *Calculate the empirical CDF's of the propensity scores for the synthetic and confidential data, separately.*

4)  *Calculate the Kolmogorov-Smirnov (KS) distance between the 2 empirical CDFs.* The KS distance is the maximum vertical distance between 2 empirical CDF distributions.

![](www/images/ks-distance.png){width="251"}
:::

<br>

-   Look at @fig-discriminant to see calculations for pMSE ratio, SPECKS, and AUC.
-   It is useful to look at [variable importance](https://topepo.github.io/caret/variable-importance.html) for predictive models when observing poor discriminant based metrics. Variable importance can help diagnose which variables are poorly synthesized.

<br>

### Specific utility metrics

::: callout-tip
### Specific utility

**Specific utility**, sometimes called outcome specific utility, measures the similarity of results for a specific analysis (or analyses) of the confidential and public data (e.g., comparing the coefficients in regression models).
:::

Specific utility metrics measure how suitable a synthetic dataset is for specific analyses.

-   These specific utility metrics will change from application to application, depending on common uses of the data.
-   A helpful rule of thumb: general utility metrics are useful for the data synthesizers to be convinced that they're doing a good job. Specific utility metrics are useful to convince downstream data users that the data synthesizers are doing a good job.

<br>

#### Recreating inferences

-   It can be useful to compare statistical analyses on the confidential data and synthetic data:
    -   Do the estimates have the same sign?
    -   Do the estimates have the same statistical inference at a common $\alpha$ level?
    -   Do the confidence intervals for the estimates overlap?
-   Each of these questions is useful. @barrientos2023feasibility combine all three questions into sign, significance, and overlap (SSO) match. SSO is the proportion of times that intervals overlap and have the same sign and significance.

<br>

#### Regression confidence interval overlap

::: callout-tip
##### Regression confidence interval overlap

**Regression confidence interval overlap** quantifies how well confidence intervals from estimates on the synthetic data recreate confidence intervals from the confidential data.

1 indicates perfect overlap (it is typically impossible to achieve a 1 on real-world data). 0 indicates intervals that are adjacent but not overlapping. Negative values indicate gaps between the intervals.

It is common to compare intervals from linear regression models and logistic regression models.
:::

![Confidence interval overlap as a measure of specific utility](www/images/confidence-interval-overlap.png)

<br> <br> <br>

The interpretability of confidence interval overlap diminishes when disclosure control methods generate very wide confidence intervals.

<br>

#### Microsimulation results

-   The Urban Institute and Tax Policy Center are heavy users of [**microsimulation**](https://www.urban.org/research/data-methods/data-analysis/quantitative-data-analysis/microsimulation#), a computer program that mimics the operation of government programs and demographic processes on individual ("micro") members of a population---people, households, or businesses, for example.

-   When synthesizing administrative tax data, we compare microsimulation results from [tax calculators](https://www.taxpolicycenter.org/resources/brief-description-tax-model) applied to the confidential data and synthetic data. The figure below shows results from the 2012 Synthetic Supplement PUF.

![Example microsimulation result comparison as a measure of specific utility](www/images/microsimulation.png) <br> <br> <br>

<br>

### Exercise: Interpreting correlation difference

::: panel-tabset
#### <font color="#55b748">**Question**</font>

```{r echo = FALSE}
library(syntheval)
corr_fit <- util_corr_fit(
  postsynth = penguins_postsynth, 
  data = penguins_conf
)

round(corr_fit$correlation_difference, digits = 3)
```

Suppose that you run another synthesis, and the correlation difference between `body_mass_g` and `bill_length_mm` **changes to 0.05**, and the correlation difference between `bill_depth_mm` and `bill_length_mm` **changes to -0.03**. Has the synthesis gotten better or worse? Why?

#### <font color="#55b748">**Solution**</font>

```{r}
round(corr_fit$correlation_difference, digits = 3)
```

Suppose that you run another synthesis, and the correlation difference between `body_mass_g` and `bill_length_mm` **changes to 0.05**, and the correlation difference between `bill_depth_mm` and `bill_length_mm` **changes to -0.03**. Has the synthesis gotten better or worse? Why?

**It depends!**

-   **The value for the first correlation has gotten further from 0 (gotten worse)**
-   **The value for the second correlation has gotten closer to 0 (gotten better)**

**To evaluate this change, you should consider which of the variable combinations is more important to get "right" according to the use case you defined, and possibly if the sign of the correlation difference matters in your policy context.**
:::

<br>

# Example results - moments

```{r}
#| echo: false
moments <- util_moments(
  postsynth = penguins_postsynth, 
  data = penguins_conf,
  group_by = species
)

moments
```

<br>

### Exercise - interpreting moments

![](www/images/sample-moments-results.png)

::: panel-tabset
#### <font color="#55b748">**Question**</font>

How would you interpret the grouped output for standard deviations? (i.e., how do different groups perform?) Is this different for the means?

#### <font color="#55b748">**Solution**</font>

How would you interpret the grouped output for standard deviations? (i.e., how do different groups perform?) Is this different for the means?

**The synthesis underestimates standard deviations for some Adalie penguins and overestimates standard deviations for some Gentoo and Chinstrap penguins. Estimates for means are closer overall, with all three species slightly overestimating at the higher end of the distribution.**
:::

<br>

<br>

### Example results - discriminant-based metrics

```{r}
#| echo: false
disc1 <- discrimination(postsynth = penguins_postsynth, data = penguins_conf)

set.seed(1)

# create recipe
# target variable and data from disc1 object
rpart_rec <- recipe(
  .source_label ~ ., 
  data = disc1$combined_data
)

# specify model and engine
rpart_mod <- decision_tree(cost_complexity = 0.01) %>%
  set_mode(mode = "classification") %>%
  set_engine(engine = "rpart")

# fit model to generate predicted probabilities
disc1 <- disc1 %>%
  add_propensities(
    recipe = rpart_rec,
    spec = rpart_mod
  ) 

disc1 %>%
  add_discriminator_auc()
```

<br>

### Exercise - interpreting AUC

::: panel-tabset
#### <font color="#55b748">**Question**</font>

Suppose you run another synthesis and the AUC from above **increases** to 0.87. Has the synthesis gotten better or worse?

#### <font color="#55b748">**Solution**</font>

Suppose you run another synthesis and the AUC from above **increases** to 0.87. Has the synthesis gotten better or worse?

**For discriminant-based metrics, you want models to perform poorly, since this indicates that the synthetic and confidential data are representative of the same population. A "better" AUC in the modeling context (closer to 1) therefore indicates a worse synthesis.**
:::

<br>

## Assessing disclosure risk

We now pivot to evaluating the disclosure risks of synthetic data. Note that most thresholds for acceptable disclosure risk are often determined by law.

There are generally three kinds of disclosure risk:

-   Identity disclosure risk;
-   Attribute disclosure risk; and
-   Inferential disclosure risk.

<br>

### Identity disclosure metrics

::: callout-tip
#### Identity disclosure

**Identity disclosure** occurs if the data intruder associates a known individual with a public data record (e.g., a record linkage attack or when a data adversary combines one or more external data sources to identify individuals in the public data).
:::

@sweeney2013identifying used voter data to re-identify individuals in the Personal Genome Project.

![Record linkage attack](www/images/identity-disclosure-risk.png){#fig-identity-disclosure-risk width="450"} <br> <br> <br>

For fully synthetic datasets, there is no one to one relationship between individuals and records so identity disclosure risk is ill-defined. Generally identity disclosure risk applies to partially synthetic datasets (or datasets protected with traditional SDC methods).

::: callout-tip
#### Identity disclosure metrics

**Identity disclosure metrics** evaluate how often we correctly re-identify confidential records in the synthetic data.

**Note:** These metrics require major assumptions about attacker information.
:::

#### Basic matching approaches

-   We start by making assumptions about the knowledge an attacker has (i.e., external publicly accessible data they have access to).

-   For each confidential record, the data attacker identifies a set of partially synthetic records which they believe contain the target record (i.e., potential matches) using the external variables as matching criteria.

-   There are distance-based and probability-based algorithms that can perform this matching. This matching process could be based on exact matches between variables or some relaxations (i.e., matching continuous variables within a certain radius of the target record, or matching adjacent categorical variables).

-   We then evaluate how accurate our re-identification process was using a variety of metrics.

<br>

::: panel-tabset
##### Expected Match Rate

-   **Expected Match Rate**: On average, how likely is it to find a "correct" match among all potential matches? Essentially, the expected number of observations in the confidential data expected to be correctly matched by an intruder.

    -   Higher expected match rate = higher identification disclosure risk.

    -   The two other risk metrics below focus on the subset of confidential records for which the intruder identifies a single match.

##### True Match Rate

-   **True Match Rate**: The proportion of true unique matches among all confidential records. Higher true match rate = higher identification disclosure risk.

##### False Match Rate

-   **False Match Rate**: The proportion of false matches among the set of unique matches. Lower false match rate = higher identification disclosure risk.
:::

<br>

### Attribute disclosure metrics

It is possible to learn confidential attributes without perfectly re-identifying observations in the data.

::: callout-tip
### Attribute disclosure

**Attribute disclosure** occurs if the data intruder determines new characteristics (or attributes) of an individual based on the information available through public data or statistics (e.g., if a dataset shows that all people age 50 or older in a city are on Medicaid, then the data adversary knows that any person in that city above age 50 is on Medicaid). This information is learned without idenfying a specific individual in the data!
:::

<br>

#### Predictive Accuracy

::: callout-tip
#### Predictive Accuracy

**Predictive accuracy** measures how well an attacker can learn about attributes in the confidential data using the synthetic data (and possibly external data).
:::

-   Similar to above, you start by matching synthetic records to confidential records. Alternatively, you can build a predictive model using the synthetic data to make predictions on the confidential data.

-   **key variables**: Variables that an attacker already knows about a record and can use to match.

-   **target variables**: Variables that an attacker wishes to know more or infer about using the synthetic data.

-   Pick a sensitive variable in the confidential data and use the synthetic data to make predictions. Evaluate the accuracy of the predictions.

<br>

#### Membership Inference Tests

::: callout-tip
#### Memebership Inference Test

**Membership inference tests** explore how well an attacker can determine if a given observations was in the training data for the synthetic data.
:::

-   Why is this important? Sometimes membership in a synthetic dataset is also confidential (e.g., a dataset of HIV positive patients or people who have experienced homelessness).

-   Also particularly useful for fully synthetic data where identity disclosure and attribute disclosure metrics don't really make a lot of sense.

-   Assumes that attacker has access to a subset of the confidential data, and wants to tell if one or more records was used to generate the synthetic data.

-   Since we as data maintainers know the true answers, we can evaluate whether the attackers guess is true and can break it down many ways (e.g., true positives, true negatives, false positives or false negatives).

![Source: @mendelevitch2021fidelity](www/images/membership-inference-test.png){width="688"}

-   The "close enough" threshold is usually determined by a custom distance metric, like edit distance between text variables or numeric distance between continuous variables.

-   Often you will want to choose different distance thresholds and evaluate how your results change.

<br>

#### Copy Protection

::: callout-tip
#### Copy Protection Metrics

**Copy protection metrics** measure how often the synthesizer memorizes or inadvertantly duplicates confidential records.
:::

**Duplicate** records, or records that have been exactly copied after synthesis, can be evaluated across the original and synthetic data.

-   ***Distance to Closest record***: Measures distance between each real record ($r$) and the closest synthetic record ($s_i$), as determined by a distance calculation.
    -   Many common distance metrics used in the literature including Euclidean distance, cosine distance, Gower distance, or Hamming distance [@mendelevitch2021fidelity].
    -   The goal of this metric is to easily expose exact copies or simple perturbations of the real records that exist in the synthetic dataset.
-   The counts of duplicated records, as well as the share of the total number of records, can be evaluated for subpopulations of the duplicated records against the same subpopulations in the full data.
    -   This can help evaluate if certain populations are subjected to higher duplication rates (and potentially higher disclosure rates) than other populations.
-   **Unique-unique** records refer to a special type of duplicated record that is completely unique (the combination of record values only appears once) in both the confidential and synthetic data.
    -   These records can also be evaluated by counts, share of the total, and characteristics of the populations represented in these records.

<br>

#### Holdout data

-   Membership inference tests and copy protection metrics are informative but lack context.
-   When possible, create a holdout data set similar to the training data. Then calculate membership inference tests and copy protections metrics replacing the synthetic data with the hold out data.
-   The results are useful for benchmarking the original membership inference tests and copy protection metrics.

<br>

### Inferential disclosure

::: callout-tip
### Inferential disclosure

**Inferential disclosure** occurs if the data intruder predicts the value of some characteristic from an individual more accurately with the public data or statistic than would otherwise have been possible (e.g., if a public homeownership dataset reports a high correlation between the purchase price of a home and family income, a data adversary could infer another person's income based on purchase price listed on Redfin or Zillow).
:::

Inferential disclosure is a specialized type of attribute disclosure, so the metrics discussed above apply here as well. Inference disclosure risk is very hard to predict, so many federal agencies tend to disregard this type of risk.

<br>

## Comparing many syntheses

To assess the privacy-utility tradeoff for many syntheses, we recommend:

-   combining the chosen metrics and corresponding visualizations into a unified report for each synthesis;
-   automating the process of generating these reports after each synthesis.

We have had the most success using R Markdown or Quarto for both aspects of this process. We discuss methodology we used in the past to compare syntheses with R Markdown in greater detail in [this blog post](https://urban-institute.medium.com/communicating-synthetic-data-evaluation-results-with-a-parameterized-r-markdown-report-8367dd976ee3).
