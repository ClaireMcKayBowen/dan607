---
title: "Data Privacy and Data Synthesis Overview"
format:
  html:
    toc: true
    code-line-numbers: true
editor_options:
  chunk_output_type: console
execute:
  warning: false
  message: false
bibliography: references.bib
---

```{r}
#| label: setup
#| echo: false
#| message: false

library(tidyverse)
library(palmerpenguins)
library(kableExtra)
library(gt)
library(urbnthemes)

set_urbn_defaults(style = "print")

options(scipen = 999)

#source(here::here("R", "create_table.R"))
exercise_number <- 1

```

## Why is Data Privacy important?

-   Modern computing and technology has made it easy to collect and process large amounts of data.
-   Malicious actors can use computing power and advanced techniques to reidentify individuals by linking supposedly anonymized records with public databases.
-   This kind of attack is called a "record linkage" attack. The following are some examples of famous record linkage attacks.
    -   In 1997, MA Gov. Bill Weld announced the public release of insurance data for researchers. He assured the public that PII had been deleted. A few days later, Dr. Latanya Sweeney, then a MIT graduate student, mailed to Weld's office his personal medical information. She purchased voter data and linked Weld's birth date, gender, and zip code to his health records. And this was back in 1997, when computing power was miniscule, and social media didn't exist!
    -   A study by Dr. Latayna Sweeney based on the 1990 Census [@sweeney2000simple] found that the 87% of the US population had reported characteristics that likely made them unique based only on ZIP, gender, and date of birth.
-   Malicious actors can also reconstruct databases or by reconstructing databases from statistics
-   Releasing granular data can be of immense value to researchers. For example, cell phone data are invaluable for emergency responses to natural disasters, and granular medical data will lead to better treatment and development of cures.

![](www/images/racial_data_covid.png){width="600"}

-   More granular data are also important for understanding equity, particularly for smaller populations and subgroups.

-   There are at least three major threats to data privacy.

    1.  **Hackers:** adversaries who steal confidential information through unauthorized access.\
    2.  **Snoopers:** adversaries who reconstruct confidential information from data releases.
    3.  **Hoarders:** stewards who collect data but don't release the data even if respondents want the information releasesd.

-   There are differing notions of what should and shouldn't be private, which may include being able to opt out of or opt into disclosure protections.

-   Data privacy is a broad topic, which includes data security, encryption, access to data, etc. We will not be covering privacy breaches from unauthorized access to a database (e.g., hackers).

-   We are instead focused on privacy preserving access to data.

-   Although data privacy and data confidentiality are certainly related, they are different, and both play a role in limiting statistical disclosure risk.

### What do we mean by data privacy?

::: callout-tip
### Data Privacy

**Data Privacy** is the ability "to determine what information about ourselves we will share with others." [@fellegi1972question]. Data privacy is a broad topic, which includes data security, encryption, access to data, etc. We will not be covering privacy breaches from unauthorized access to a database (e.g., hackers). **We are instead focused on privacy preserving access to data.**
:::

::: callout-tip
## Confidentiality

**Confidentiality** is "the agreement, explicit or implicit, between data subject and data collector regarding the extent to which access by others to personal information is allowed" [@fienberg2018statistical].
:::

::: callout-tip
### Statistical Disclosure Control

**Statistical Disclosure Control (SDC)** or Statistical Disclosure Limitation (SDL) is a field of study that aims to develop methods for releasing high-quality data products while preserving data confidentiality as a means of maintaining privacy. **Synthetic data is an SDC method.**
:::

## Overview of SDC

-   SDC methods have existed within statistics and the social sciences since the mid-twentieth century.

-   Below is an opinionated, and incomplete, overview of various SDC methods.

<br>

### SDC Methods

-   Here are a few traditional methods from the SDC literature. See @matthews2011data for more information.

    -   **Suppression:** Not releasing data about certain subgroups or witholding information about certaint observations.

    -   **Swapping:** The exchange of sensitive values among sample units with similar characteristics.

    -   **Generalization:** Aggregating variables into larger units (e.g., reporting state rather than zip code) or top/bottom coding (limiting values below/above a threshold to the threshold value).

    -   **Noise Infusion:** Adding random noise, often to continuous variables which can maintain univariate distributions.

    -   **Sampling:** Only releasing a sample of individual records.

-   The problem with the above approaches is that they really limit the utility of the data.

    -   @mitrareiter found that a 5 percent swapping of 2 identifying variables in the 1987 Survey of Youth in Custody invalidated statistical hypothesis tests in regression.

    -   Top/bottom coding eliminates information at the tails of the distributions, degrading analyses that depend on the entire distribution [@Reiter_Wang_Zhang_2014].


Synthetic data can help overcome some of these issues. We will also discuss formal privacy, a framework for methods that can help quantify the privacy-utility tradeoff.

## Defining key data privacy stakeholders

![Key stakeholders in the privacy ecosystem](www/images/stakeholders.png){#fig-stakeholders fig-align="center"}

## What is the privacy-utility tradeoff?

::: callout-tip
### Data Utility

**Data utility,** quality, accuracy, or usefulness is how practically useful or accurate to the data are for research and analysis purposes.
:::

There is often a tension between privacy and data utility. This tension is referred to in the privacy literature as the **privacy-utility tradeoff**.

![*Generally*, as privacy increases, the image quality (utility) decreases, and vice versa.](www/images/privacy-utility-tradeoff.png){#fig-tradeoff fig-align="center"}

## What is synthetic data? Why use it?

::: callout-tip
### Synthetic Data

**Synthetic data** consist of pseudo or "fake" records that are statistically representative of the confidential data.
:::

-   The goal of most syntheses is to closely mimic the underlying distribution and statistical properties of the real data to preserve data utility while minimizing disclosure risks.
-   Synthesized values also limit an intruder's confidence, because they cannot confirm a synthetic value exists in the confidential dataset.
-   Synthetic data may be used as a "training dataset" to develop programs to run on confidential data via a validation server.

::: callout-tip
### Partially synthetic

**Partially synthetic** data only synthesizes some of the variables in the released data (generally those most sensitive to disclosure). In partially synthetic data, there remains a one-to-one mapping between confidential records and synthetic records.
:::

Below, we see an example of what a partially synthesized version of confidential data could look like.

![Partially synthetic data](www/images/partially-synthetic-data.png){#fig-partial width="550"}

::: callout-tip
### Fully synthetic

**Fully synthetic** data synthesizes all values in the dataset with imputed amounts. Fully synthetic data no longer directly map onto the confidential records, but remain statistically representative. Since fully synthetic data does not contain any actual observations, it protects against both attribute and identity disclosure.
:::

Below, we see an example of what a fully synthesized version of confidential data might look like.

![Fully synthetic data](www/images/fully-synthetic-data.png){#fig-fully width="550"}

### Partial vs. fully synthetic advantages and disadvantages

-   Changing only some variables (partial synthesis) in general leads to higher utility in analysis since the relationships between variables are by definition unchanged (Drechsler et al, 2008).

-   Disclosure in fully synthetic data is nearly impossible because all values are imputed, while partial synthesis has higher disclosure risk since confidential values remain in the dataset (Drechsler et al, 2008).

    -   Note that while the risk of disclosure for fully synthetic data is very low, it is not zero.

-   Accurate and exhaustive specification of variable relationships and constraints in fully synthetic data is difficult and if done incorrectly can lead to bias @drecshlerjorgcomparingsynthetic.

    -   If a variable is synthesized incorrectly early in a sequential synthesis, all variables synthesized on the basis of that variable will be affected.

-   Partially synthetic data may be publicly perceived as more reliable than fully synthetic data.

## Exercise

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

Consider this penguins data:

```{r}
#| echo: false

set.seed(125)

ex_data <- penguins |> 
  select(species, bill_length_mm, sex) |> 
  slice_sample(n = 5) 

ex_data |> 
  gt()
```

Let's say that researchers decide that the `sex` of the penguins in the data are not confidential, but the `species` and `bill length` are. So, they develop regression models that predict `species` conditional on `sex` and predict `bill_length` conditional on `species` and `sex`. They then use those models to predict species and bill lengths for each row in the data and then release it publicly.

::: panel-tabset
### <font color="#55b748">Question</font>

*What specific Statistical Disclosure Control method are these researchers using?*

### <font color="#55b748">Solution</font>

*What specific Statistical Disclosure Control method are these researchers using?*

They are using partially synthetic data.
:::

## Exercise

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

::: panel-tabset
### <font color="#55b748">Question</font>

*A researcher has confidential data on a population. To protect the privacy of respondents, the researcher releases a synthetic version of the data. A data attacker then runs a record linkage attack against the synthetic data and is able to accurately identify 5 individuals in the data. Based on this information, can you tell whether the researcher released fully or partially synthetic data? Why or why not?*

### <font color="#55b748">Answer</font>

*A researcher has confidential data on a population. To protect the privacy of respondents, the researcher releases a synthetic version of the data. A data attacker then runs a record linkage attack against the synthetic data and is able to accurately identify 5 individuals in the data. Based on this information, can you tell whether the researcher released fully or partially synthetic data? Why or why not?*

Record linkage attacks are only possible for partially synthetic data, though other types of disclosure risk still apply to fully synthetic data.
:::

### Why synthetic data?

Synthetic data provides enhanced disclosure protection with a lower cost to utility than other "traditional" SDC methods. For example:

-   @mitrareiter found that a 5 percent swapping of 2 identifying variables in the 1987 Survey of Youth in Custody invalidated statistical hypothesis tests in regression.

-   Top/bottom coding eliminates information at the tails of the distributions, degrading analyses that depend on the entire distribution [@Reiter_Wang_Zhang_2014].

It also allows for release of data that is more disaggregated than might otherwise be possible with "traditional" SDC (aggregation is a very common SDC technique).

## Data Synthesis Process Overview

Note that this overview is opinionated and simplified in order to provide a reasonable summary.

![The synthesis process is very iterative, particularly in the privacy step](www/images/synthesis-process-iterative.png){#fig-process-iterative}

### Privacy stakeholders and the synthesis process

![All of the privacy stakeholders discussed previously have a role in aspects of the synthesis process](www/images/synthesis-process-actors.png){#fig-process-actors}

For more on involving data users and data participants in the synthesis process, we recommend [Do No Harm Guide: Applying Equity Awareness in Data Privacy Methods](https://www.urban.org/research/publication/do-no-harm-guide-applying-equity-awareness-data-privacy-methods), a report by Claire Bowen and Joshua Snoke.

## Key terms for synthesis process

In a perfect world, we would synthesize data by directly modeling the joint distribution of the variables of interest. Unfortunately, this is often computationally infeasible.

Instead, we often decompose a joint distribution into a sequence of conditional distributions.

::: callout-tip
### Sequential synthesis

**Sequential synthesis** is a more advanced implementation of synthetic data generation that iteratively estimates models for each predictor with previously synthesized variables used as predictors.
:::

Sequential synthesis allows us to easily model multivariate relationships without being computationally expensive, and is the methodology used by `tidysynthesis`.

-   We can select the synthesis order based on the priority of the variables or the relationships between them.
-   The earlier in the order a variable is synthesized, the better the original information is preserved in the synthetic data **usually**.
-   [@bowen2021differentially] proposed a method that ranks variable importance by either practical or statistical utility and sequentially synthesizes the data accordingly.

The process described above may be easier to understand with the following table:

```{r, echo = FALSE}
table = tribble(~Step, ~`Synthesized Outcome`, ~`Modelled with`, ~`Synthetic values predicted with`,
                "1", "Sex", '-', "Random sampling with replacement",
                "2", "Age", "Fit model of Age using confidential Sex", "Synthesized Sex",
                "3", "Social Security Benefits","Fit model of Social Security Benefits using confidential Age, Sex" , "Synthesized Sex, synthesized Age",
                '-', '-', '-', '-',
                )
table |> 
  gt()
```

::: callout-tip
### Implicates

Researchers can create any number of versions of a partially synthetic or fully synthetic dataset. Each version of the dataset is called an **implicate**. These can also be referred to as replicates or simply "synthetic datasets."
:::

-   For partially synthetic data, non-synthesized variables are the same across each version of the dataset.

-   Multiple implicates are useful for understanding the uncertainty added by imputation and are required for calculating valid standard errors.

-   More than one implicate can be released for public use; each new release, however, increases disclosure risk (but allows for more complete analysis and better inferences, provided users use the correct combining rules).

-   Implicates can also be analyzed internally to find which version(s) of the dataset provide the most utility in terms of data quality.

## Exercise: Sequential synthesis

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

::: panel-tabset
#### <font color="#55b748">**Question**</font>

You have a confidential dataset that contains information about dogs' `weight` and their `height`. You decide to sequentially synthesize these two variables and write up your method below. Can you spot the mistake in writing up your method?

> To create a synthetic record, first synthetic pet weight is assigned based on a random draw from a normal distribution with mean equal to the average of confidential weights, and standard deviation equal to the standard deviation of confidential weights. Then the confidential `height` is regressed on the synthetic `weight`. Using the resulting regression coefficients, a synthetic `height` variable is generated for each row in the data using just the synthetic `weight` values as an input.

#### <font color="#55b748">**Answer**</font>

You have a confidential dataset that contains information about dogs' `weight` and their `height`. You decide to sequentially synthesize these two variables and write up your method below. Can you spot the mistake in writing up your method?

> To create a synthetic record, first synthetic pet weight is assigned based on a random draw from a normal distribution with mean equal to the average of confidential weights, and standard deviation equal to the standard deviation of confidential weights. Then the confidential `height` is regressed on the synthetic `weight`. Using the resulting regression coefficients, a synthetic `height` variable is generated for each row in the data using just the synthetic `weight` values as an input.

**`Height` should be regressed on the confidential values for `weight`, rather than the synthetic values for `weight`**
:::

## Exercise: Multiple implicates

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

::: panel-tabset
#### <font color="#55b748">**Question**</font>

*What are the privacy implications for releasing multiple versions of a synthetic dataset (implicates)? Do these implications change for partially vs. fully synthetic data?*

#### <font color="#55b748">**Notes**</font>

*What are the privacy implications for releasing multiple versions of a synthetic dataset (implicates)? Do these implications change for partially vs. fully synthetic data?*

-   Releasing multiple implicates improves transparency and analytical value, but increases disclosure risk (violates "security through obscurity").

-   It is more risky to release partially synthetic implicates, since non-synthesized records are the same across each dataset and there remains a 1-to-1 relationship between confidential and synthesized records.
:::

## Exercise: Partial vs. fully synthetic

```{r}
#| echo: false

exercise_number <- exercise_number + 1
```

Shown here are the first seven rows of a dataset about the prices and attributes of diamonds. Suppose you decide to synthesize the "price" variable, because that information is too sensitive for public release.

| price | carat | cut       | color | clarity |
|-------|-------|-----------|-------|---------|
| 326   | 0.23  | Ideal     | E     | SI2     |
| 326   | 0.21  | Premium   | E     | SI1     |
| 327   | 0.23  | Good      | E     | VS1     |
| 334   | 0.29  | Premium   | I     | VS2     |
| 335   | 0.31  | Good      | J     | SI2     |
| 336   | 0.24  | Very Good | J     | VVS2    |
| 336   | 0.24  | Very Good | I     | VVS1    |

::: panel-tabset
#### <font color="#55b748">**Question**</font>

*After you synthesize the price variable, would the resulting dataset be considered partially or fully synthetic?*

*What are the trade-offs of a partially synthetic dataset compared to a fully synthetic dataset?*

*Describe in words how you would synthesize the "price" variable. Is the method you described parametric or non-parametric? Why?*

#### <font color="#55b748">**Notes**</font>

*After you synthesize the price variable, would the resulting dataset be considered partially or fully synthetic?*

Partially synthetic

*What are the trade-offs of a partially synthetic dataset compared to a fully synthetic dataset?*

-   Changing only some variables (partial synthesis) in general leads to higher utility in analysis since the relationships between variables are by definition unchanged (Drechsler et al, 2008).

-   Disclosure in fully synthetic data is nearly impossible because all values are imputed, while partial synthesis has higher disclosure risk since confidential values remain in the dataset (Drechsler et al, 2008).

    -   Note that while the risk of disclosure for fully synthetic data is very low, it is not zero.

-   Accurate and exhaustive specification of variable relationships and constraints in fully synthetic data is difficult and if done incorrectly can lead to bias (Drechsler et al, 2008).

    -   If a variable is synthesized incorrectly early in a sequential synthesis, all variables synthesized on the basis of that variable will be affected.

-   Partially synthetic data may be publicly perceived as more reliable than fully synthetic data.

*Describe in words how you would synthesize the "price" variable. Is the method you described parametric or non-parametric? Why?*
:::
