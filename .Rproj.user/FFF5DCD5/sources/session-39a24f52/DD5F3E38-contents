---
title: "Key Definitions and Synthesis Process Overview"
output:
  html:
    toc: true
    embed-resources: true
    code-line-numbers: true
editor_options:
  chunk_output_type: console
execute:
  warning: false
  message: false
bibliography: references.bib
---

```{r}
#| label: setup
#| echo: false
#| message: false

library(tidyverse)
library(palmerpenguins)
library(kableExtra)
library(gt)
library(urbnthemes)

set_urbn_defaults(style = "print")

options(scipen = 999)

source(here::here("R", "create_table.R"))

```

## Definitions, decision points, and iteration opportunities

As we present information about the data synthesis process, we want to highlight:

-   key definitions;
-   opportunities to make decisions that will define the course of the overall synthesis; and
-   opportunities to iterate on decisions about the data and models you use for synthesis.

We will use the following callout boxes to highlight these moments.

::: callout-tip
### Definitions

Definitions of key terminology will be highlighted in boxes like this one.
:::

::: callout-important
### Decision points and iteration opportunities

Boxes like this one will highlight opportunities to make decisions that affect the overall synthesis process and results. Both types of opportunities are opinionated and flexible, but "iteration opportunities" pose more flexibility and are easier to update.
:::

## What do we mean by data privacy?

::: callout-tip
### Data Privacy

**Data Privacy** is the ability "to determine what information about ourselves we will share with others." [@fellegi1972question]. Data privacy is a broad topic, which includes data security, encryption, access to data, etc. We will not be covering privacy breaches from unauthorized access to a database (e.g., hackers). **We are instead focused on privacy preserving access to data.**
:::

::: callout-tip
### Statistical Disclosure Control

**Statistical Disclosure Control (SDC)** or Statistical Disclosure Limitation (SDL) is a field of study that aims to develop methods for releasing high-quality data products while preserving data confidentiality as a means of maintaining privacy. **Synthetic data is an SDC method.**
:::

## Defining key data privacy stakeholders

![Key stakeholders in the privacy ecosystem](www/images/stakeholders.png){#fig-stakeholders fig-align="center"}

## What is the privacy-utility tradeoff?

::: callout-tip
### Data Utility

**Data utility,** quality, accuracy, or usefulness is how practically useful or accurate to the data are for research and analysis purposes.
:::

There is often a tension between privacy and data utility. This tension is referred to in the privacy literature as the **privacy-utility tradeoff**.

![*Generally*, as privacy increases, the image quality (utility) decreases, and vice versa.](www/images/privacy-utility-tradeoff.png){#fig-tradeoff fig-align="center"}

## What is synthetic data? Why use it?

::: callout-tip
### Synthetic Data

**Synthetic data** consist of pseudo or "fake" records that are statistically representative of the confidential data.
:::

-   The goal of most syntheses is to closely mimic the underlying distribution and statistical properties of the real data to preserve data utility while minimizing disclosure risks.
-   Synthesized values also limit an intruder's confidence, because they cannot confirm a synthetic value exists in the confidential dataset.
-   Synthetic data may be used as a "training dataset" to develop programs to run on confidential data via a validation server.

::: callout-tip
### Partially synthetic

**Partially synthetic** data only synthesizes some of the variables in the released data (generally those most sensitive to disclosure). In partially synthetic data, there remains a one-to-one mapping between confidential records and synthetic records.
:::

Below, we see an example of what a partially synthesized version of confidential data could look like.

![Partially synthetic data](www/images/partially-synthetic-data.png){#fig-partial width="550"}

::: callout-tip
### Fully synthetic

**Fully synthetic** data synthesizes all values in the dataset with imputed amounts. Fully synthetic data no longer directly map onto the confidential records, but remain statistically representative. Since fully synthetic data does not contain any actual observations, it protects against both attribute and identity disclosure.
:::

Below, we see an example of what a fully synthesized version of confidential data might look like.

![Fully synthetic data](www/images/fully-synthetic-data.png){#fig-fully width="550"}

### Partial vs. fully synthetic advantages and disadvantages

- Changing only some variables (partial synthesis) in general leads to higher utility in analysis since the relationships between variables are by definition unchanged (Drechsler et al, 2008).

- Disclosure in fully synthetic data is nearly impossible because all values are imputed, while partial synthesis has higher disclosure risk since confidential values remain in the dataset (Drechsler et al, 2008).
  
  - Note that while the risk of disclosure for fully synthetic data is very low, it is not zero.

- Accurate and exhaustive specification of variable relationships and constraints in fully synthetic data is difficult and if done incorrectly can lead to bias [@drecshlerjorgcomparingsynthetic].
  
  - If a variable is synthesized incorrectly early in a sequential synthesis, all variables synthesized on the basis of that variable will be affected.

- Partially synthetic data may be publicly perceived as more reliable than fully synthetic data.

### Why synthetic data?

Synthetic data provides enhanced disclosure protection with a lower cost to utility than other "traditional" SDC methods. For example:

-   Mitra and Reiter (2006) found that a 5 percent swapping of 2 identifying variables in the 1987 Survey of Youth in Custody invalidated statistical hypothesis tests in regression.

-   Top/bottom coding eliminates information at the tails of the distributions, degrading analyses that depend on the entire distribution (Fuller 1993; Reiter, Wang, and Zhang, 2014).

It also allows for release of data that is more disaggregated than might otherwise be possible with "traditional" SDC (aggregation is a very common SDC technique).

## Data Synthesis Process Overview

Note that this overview is opinionated and simplified in order to provide a reasonable summary.

![The synthesis process is very iterative, particularly in the privacy step](www/images/synthesis-process-iterative.png){#fig-process-iterative}

### Privacy stakeholders and the synthesis process

![All of the privacy stakeholders discussed previously have a role in aspects of the synthesis process](www/images/synthesis-process-actors.png){#fig-process-actors}

For more on involving data users and data participants in the synthesis process, we recommend [Do No Harm Guide: Applying Equity Awareness in Data Privacy Methods](https://www.urban.org/research/publication/do-no-harm-guide-applying-equity-awareness-data-privacy-methods), a report by Claire Bowen and Joshua Snoke.

## Key terms for synthesis process

In a perfect world, we would synthesize data by directly modeling the joint distribution of the variables of interest. Unfortunately, this is often computationally infeasible.

Instead, we often decompose a joint distribution into a sequence of conditional distributions.

::: callout-tip
### Sequential synthesis

**Sequential synthesis** is more advanced implementation of synthetic data generation that iteratively estimates models for each predictor with previously synthesized variables used as predictors.
:::

Sequential synthesis allows us to easily model multivariate relationships without being computationally expensive, and is the methodology used by `tidysynthesis`.

- We can select the synthesis order based on the priority of the variables or the relationships between them.
- The earlier in the order a variable is synthesized, the better the original information is preserved in the synthetic data **usually**.
- [@bowen2021differentially] proposed a method that ranks variable importance by either practical or statistical utility and sequentially synthesizes the data accordingly.

The process described above may be easier to understand with the following table:

```{r, echo = FALSE}
table = tribble(~Step, ~Outcome, ~`Modelled with`, ~`Predicted with`,
                "1", "Sex", '-', "Random sampling with replacement",
                "2", "Age", "Sex", "Sampled Sex",
                "3", "Social Security Benefits","Sex, Age" , "Sampled Sex, Sampled Age",
                '-', '-', '-', '-',
                )
table |> 
  gt()
```

::: callout-tip
### Implicates

Researchers can create any number of versions of a partially synthetic or fully synthetic dataset. Each version of the dataset is called an **implicate**. These can also be referred to as replicates or simply "synthetic datasets."
:::

-   For partially synthetic data, non-synthesized variables are the same across each version of the dataset.

-   Multiple implicates are useful for understanding the uncertainty added by imputation and are required for calculating valid standard errors.

-   More than one implicate can be released for public use; each new release, however, increases disclosure risk (but allows for more complete analysis and better inferences, provided users use the correct combining rules).

-   Implicates can also be analyzed internally to find which version(s) of the dataset provide the most utility in terms of data quality.
